{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install git+https://github.com/huggingface/transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-VL-4B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T18:40:14.808304Z","iopub.execute_input":"2025-10-30T18:40:14.809169Z","iopub.status.idle":"2025-10-30T18:41:07.357059Z","shell.execute_reply.started":"2025-10-30T18:40:14.809140Z","shell.execute_reply":"2025-10-30T18:41:07.355971Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2753c968c55c4273b9fb3ce650a46ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01552dbbca984838a8eac1cd01ed9bf1"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def generate_fun(title, rendered_text, image_base64):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f'''Title: {title}\nRendered Text: {rendered_text}\n\nUse the image and the title and rendered text to create a prompt that can be used as an instruction for image generation models to generate the provided image. \nDon't add too much detail. One example instruction can be:\n\"Generate an image with the background as .......... and text '.......' written on it.\"\nDon't make the prompt too long ‚Äî maximum 30 words only. \nAdd quotes around the text to be written on the image; everything else should be outside quotes. Also the text that we want to print will be in top to bottom order from image.\nMost important point is that the text that we want to print will be from `Rendered Text` only, if the `Rendered Text` does not contain any text then dont print any text also if all the text presented in the `Rendered Text` should be there in generated instruction. The prompt should sound natural and do not mention the position of the text.''',\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"},\n                },\n            ],\n        }\n    ]\n\n    # Preparation for inference\n    inputs = processor.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    )\n    inputs = inputs.to(model.device)\n    \n    # Inference: Generation of the output\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    \n    del inputs\n    torch.cuda.empty_cache()\n    \n    return output_text\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T19:04:10.275843Z","iopub.execute_input":"2025-10-30T19:04:10.276870Z","iopub.status.idle":"2025-10-30T19:04:10.284420Z","shell.execute_reply.started":"2025-10-30T19:04:10.276828Z","shell.execute_reply":"2025-10-30T19:04:10.283426Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"ds['train'][20]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import base64\nfrom io import BytesIO\nfrom openai import OpenAI\nfrom datasets import DatasetDict, load_dataset\nimport torch\nds=load_dataset(\"Pulkit996/Pulkit_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T18:42:23.437374Z","iopub.execute_input":"2025-10-30T18:42:23.438425Z","iopub.status.idle":"2025-10-30T18:42:24.114681Z","shell.execute_reply.started":"2025-10-30T18:42:23.438386Z","shell.execute_reply":"2025-10-30T18:42:24.114035Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n\n# --- Define selecting_working_text ---\ndef selecting_working_text(example):\n    font_sizes = example['font_size']\n    texts = example['text']\n\n    # Pair text with font size\n    items = list(zip(font_sizes, texts))\n\n    # Sort by font size descending\n    sorted_items = sorted(items, key=lambda x: x[0], reverse=True)\n\n    # Pick up to two distinct non-empty texts\n    kept_texts = []\n    for size, text in sorted_items:\n        if text and text.strip() and text not in kept_texts:\n            kept_texts.append(text)\n        if len(kept_texts) >= 2:\n            break\n\n    # If the first text is long (>8 chars), keep only it\n    if kept_texts and len(kept_texts[0].split()) > 8:\n        kept_texts = kept_texts[:1]\n\n    # Blank out all other texts\n    new_texts = [t if t in kept_texts else \"\" for t in texts]\n    example[\"text\"] = new_texts\n    return example\n\n\n# --- Define main function ---\ndef fun(example):\n    try:\n        example = selecting_working_text(example)\n        image_data = example[\"preview\"]\n        title = example[\"title\"]\n        rendered_text = example[\"text\"]\n\n        # Convert in-memory image to bytes\n        if isinstance(image_data, bytes):\n            image_bytes = image_data\n        else:\n            buffer = BytesIO()\n            image_data.save(buffer, format=\"PNG\")\n            image_bytes = buffer.getvalue()\n\n        image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\n        \n        prompt_text = generate_fun(title, rendered_text, image_base64)\n        # print(prompt_text)\n        # print(f\"‚úÖ Generated prompt: {prompt_text}\")\n        return {\"text\": rendered_text, \"prompt\": prompt_text}\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Error processing one example: {e}\")\n        return {\"prompt\": None}\n\n\ndef add_new_column_sample(dataset_dict, num_samples=5):\n    updated = DatasetDict()\n\n    for split, dset in dataset_dict.items():\n        print(f\"\\nProcessing split: {split} ({len(dset)} samples)\")\n        # sample = dset.select(range(min(num_samples, len(dset))))  # take only few samples\n        sample=dset\n        new_data = []\n        for i, example in enumerate(sample):\n            print(f\"üß† Processing example {i+1}/{len(sample)}...\")\n            result = fun(example)  # call your function directly\n\n            if(i%10==0):\n                print(result)\n            new_data.append({**example, **result})\n\n            # clean GPU memory between runs\n            import torch\n            torch.cuda.empty_cache()\n\n        # Create a new dataset from processed examples\n        updated[split] = sample.from_dict({k: [d[k] for d in new_data] for k in new_data[0].keys()})\n        print(f\"‚úÖ Done ({len(updated[split])} samples)\")\n\n    return updated\n\n# Run it\nsampled_dataset = add_new_column_sample(ds, num_samples=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampled_dataset.save_to_disk(\"dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}